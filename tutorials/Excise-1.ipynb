{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b763560",
   "metadata": {},
   "source": [
    "# Excise 2022.11.01\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9149da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/01 10:42:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"test_SamShare\").setMaster(\"local[4]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd94eaac",
   "metadata": {},
   "source": [
    "## 1. Log file processing\n",
    "\n",
    "\n",
    "* Input: a simplified log of a web server (i.e., a textual file)\n",
    "\n",
    "    * Each line of the file is associated with a URL request\n",
    "    \n",
    "\n",
    "* Datafile path: ./data/log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4068402b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwser:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    66.249.69.97 - - [24/Sep/2014:22:25:44 +0000] \"GET http://www.google.com/bot.html”\n",
      "    66.249.69.97 - - [24/Sep/2014:22:26:44 +0000] \"GET http://www.google.com/how.html”\n",
      "    66.249.69.97 - - [24/Sep/2014:22:28:44 +0000] \"GET http://dbdmg.polito.it/course.html”\n",
      "    71.19.157.179 - - [24/Sep/2014:22:30:12 +0000] \"GET http://www.google.com/faq.html”\n",
      "    66.249.69.97 - - [24/Sep/2014:31:28:44 +0000] \"GET http://dbdmg.polito.it/thesis.html”\n",
      "    66.249.69.97 - - [24/Sep/2014:56:26:44 +0000] \"GET http://www.google.com/how.html”\n",
      "    56.249.69.97 - - [24/Sep/2014:56:26:44 +0000] \"GET http://www.google.com/how.html”\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "logRDD=sc.textFile('./data/log.txt')\n",
    "def printRDD(rdd):\n",
    "    print(\"Anwser:\")\n",
    "    tmp=rdd.collect()\n",
    "    for t in tmp:\n",
    "        print(\"   \",t)\n",
    "        \n",
    "printRDD(logRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4fcf2b",
   "metadata": {},
   "source": [
    "### 1.1  Log filtering\n",
    "\n",
    "   \n",
    "* Output: the lines containing the word “google”\n",
    "\n",
    "    \n",
    "    \n",
    "* Answer:\n",
    "        66.249.69.97 - - [24/Sep/2014:22:25:44 +0000] \"GET http://www.google.com/bot.html”\n",
    "        66.249.69.97 - - [24/Sep/2014:22:26:44 +0000] \"GET http://www.google.com/how.html”\n",
    "        71.19.157.179 - - [24/Sep/2014:22:30:12 +0000] \"GET http://www.google.com/faq.html”\n",
    "        66.249.69.97 - - [24/Sep/2014:56:26:44 +0000] \"GET http://www.google.com/how.html”\n",
    "        56.249.69.97 - - [24/Sep/2014:56:26:44 +0000] \"GET http://www.google.com/how.html”\n",
    "        \n",
    "Please write your code in the following bracket, and use \"shift+return\" to check your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0c410d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your code here:\n",
    "def contain_google(rdd):\n",
    "    # tmp = rdd.collect()\n",
    "    # for line in tmp:\n",
    "    #     if 'google' in (line):\n",
    "    #         print(line)\n",
    "\n",
    "    filter_rdd = rdd.filter(lambda line: 'google' in line)\n",
    "    #printRDD(rdd)\n",
    "    return filter_rdd\n",
    "\n",
    "contain_google(logRDD)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61678505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b7210",
   "metadata": {},
   "source": [
    "### 1.2 Log analysis\n",
    "\n",
    "* Output: the list of distinct IP addresses associated with the connections to a google page (i.e., connections to URLs containing the term “www.google.com”)\n",
    "\n",
    "\n",
    "* Answer:\n",
    "        66.249.69.97\n",
    "        71.19.157.179\n",
    "        56.249.69.97\n",
    "        \n",
    "Please write your code in the following bracket, and use \"shift+return\" to check your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a301c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwser:\n",
      "    66.249.69.97\n",
      "    66.249.69.97\n",
      "    71.19.157.179\n",
      "    66.249.69.97\n",
      "    56.249.69.97\n"
     ]
    }
   ],
   "source": [
    "# Put your code here: \n",
    "def log_analysis(rdd):\n",
    "    def get_ip(line):\n",
    "        return line.split(\" \")[0]\n",
    "\n",
    "    rdd_google = contain_google(rdd)\n",
    "    \n",
    "    printRDD(rdd_google.map(get_ip))\n",
    "\n",
    "log_analysis(logRDD)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ebf74",
   "metadata": {},
   "source": [
    "## 2. Sensor Data Processing\n",
    "\n",
    "* Input: a collection of (structured) textual csv files containing the daily value of PM10 for a set of sensors\n",
    "    * Each line of the files has the following format **\"sensorId,date,PM10 value (μg/m 3)\"**\n",
    "    \n",
    "\n",
    "* DataFile Path: ./data/sensor.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc35d1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put your code here: \n",
    "pmrdd = sc.textFile('./data/sensor.txt')\n",
    "def get_max(rdd):\n",
    "\n",
    "    rdd = rdd.map(lambda pm: float(pm.split(\",\")[-1]))\n",
    "    #rdd.map(kv)\n",
    "    return rdd.max()\n",
    "\n",
    "get_max(pmrdd)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0781bb41",
   "metadata": {},
   "source": [
    "### 2.1 Maximum value\n",
    "\n",
    "* Output: report the maximum value of PM10\n",
    "\n",
    "\n",
    "* Example:\n",
    "        60.2\n",
    "        \n",
    "Please write your code in the following bracket, and use \"shift+return\" to check your answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2bf885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwser:\n",
      "    (20.4, ('s2', '2016-01-02'))\n",
      "    (20.5, ('s1', '2016-01-01'))\n",
      "    (30.1, ('s2', '2016-01-02'))\n",
      "    (44.7, ('s3', '2016-01-01'))\n",
      "    (52.5, ('s2', '2016-01-03'))\n",
      "    (55.5, ('s1', '2016-01-03'))\n",
      "    (60.2, ('s1', '2016-01-01'))\n",
      "    (60.2, ('s1', '2016-01-03'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    }
   ],
   "source": [
    "### Put your code here: \n",
    "def get_max2(rdd):\n",
    "    def kv(line):\n",
    "        values = line.split(\",\")\n",
    "        return (float(values[-1]),(values[0],values[1]))\n",
    "    \n",
    "    rdd = rdd.map(kv)\n",
    "    rdd = rdd.sortByKey(ascending=True)\n",
    "    printRDD(rdd)\n",
    "\n",
    "get_max2(pmrdd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e28c1c",
   "metadata": {},
   "source": [
    "### 2.2 Top-k maximum values\n",
    "\n",
    "* Output: report the top-3 maximum values of PM10\n",
    "\n",
    "\n",
    "* set: k=3\n",
    "\n",
    "\n",
    "* Answer:\n",
    "        60.2\n",
    "        60.2\n",
    "        55.5\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9995787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60.2', '60.2', '55.5']\n"
     ]
    }
   ],
   "source": [
    "### Put your code here: \n",
    "def get_top3(rdd):\n",
    "\n",
    "    rdd = rdd.map(lambda pm: pm.split(\",\")[-1])\n",
    "    #rdd.map(kv)\n",
    "    return rdd.top(3)\n",
    "\n",
    "print(get_top3(pmrdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2497f4b0",
   "metadata": {},
   "source": [
    "### 2.3 Readings associated with the maximum value\n",
    "\n",
    "* Output: the line(s) associated with the maximum value of PM10\n",
    "\n",
    "\n",
    "* Answer: \n",
    "        s1,2016-01-02,60.2 \n",
    "        s1,2016-01-03,60.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2eabaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwser:\n",
      "    ('s1,2016-01-01,60.2', 60.2)\n",
      "    ('s1,2016-01-03,60.2', 60.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    }
   ],
   "source": [
    "# put your code here:\n",
    "def get_max3(rdd):\n",
    "    def kv(line):\n",
    "        values = line.split(\",\")\n",
    "        return (line,float(values[-1]))\n",
    "\n",
    "    pair_rdd = rdd.map(kv)\n",
    "    max_pm = get_max(rdd)\n",
    "    rdd = pair_rdd.filter(lambda line: line[1] == max_pm)\n",
    "    #rdd = rdd.sortByKey(ascending=True)\n",
    "    printRDD(rdd)\n",
    "\n",
    "get_max3(pmrdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ab72c",
   "metadata": {},
   "source": [
    "### 2.4 Dates associated with the maximum value\n",
    "\n",
    "* Output: the date(s) associated with the maximum value of PM10\n",
    "\n",
    "\n",
    "* Answer:\n",
    "        2016-01-01 \n",
    "        2016-01-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cefdbc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwser:\n"
     ]
    }
   ],
   "source": [
    "# Put your code here: \n",
    "def get_max4(rdd):    \n",
    "    def filter(line):\n",
    "        if line[1] == max_pm:\n",
    "            return line[0].split(\",\")[1]\n",
    "\n",
    "    max_pm = get_max(rdd)\n",
    "\n",
    "    rdd = rdd.filter(filter)\n",
    "    printRDD(rdd)\n",
    "\n",
    "\n",
    "get_max4(pmrdd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8c3803",
   "metadata": {},
   "source": [
    "### 2.5 Average Value\n",
    "\n",
    "* Output: compute the average PM10 value\n",
    "\n",
    "\n",
    "* Answer:\n",
    "        43.0125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e9a8863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.0125"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put your code here: \n",
    "def get_mean(rdd):\n",
    "    return rdd.map(lambda line: float(line.split(\",\")[-1])).mean()\n",
    "\n",
    "\n",
    "get_mean(pmrdd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f5d3bf",
   "metadata": {},
   "source": [
    "### 2.6 Maximum values\n",
    "\n",
    "* Output: the maximum value of PM10 for each sensor\n",
    "\n",
    "* Answer:\n",
    "        ('s1', 60.2)\n",
    "        ('s3', 44.7)\n",
    "        ('s2', 52.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40c2ae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwser:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ('s1', 60.2)\n",
      "    ('s3', 44.7)\n",
      "    ('s2', 52.5)\n"
     ]
    }
   ],
   "source": [
    "# Put your code here:  \n",
    "def get_max6(rdd):\n",
    "    def kv(line):\n",
    "        values = line.split(\",\")\n",
    "        return (values[0],float(values[-1]))\n",
    "\n",
    "    def cmp(d1,d2):\n",
    "        if d1>d2:\n",
    "            return d1\n",
    "        else:\n",
    "            return d2\n",
    "\n",
    "    pair_rdd = rdd.map(kv)\n",
    "    result = pair_rdd.reduceByKey(cmp)#.mapValues(lambda v: max(v, key=lambda x: len(x[1])))\n",
    "    #rdd = rdd.sortByKey(ascending=True)\n",
    "    printRDD(result)\n",
    "\n",
    "get_max6(pmrdd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cd3c80",
   "metadata": {},
   "source": [
    "### 2.7 Pollution analysis\n",
    "\n",
    "* Output: the sensors with at least 2 readings with a PM10 value greater than the critical threshold 50\n",
    "    * Store in an HDFS file the sensorIds of the selected sensors and also the number of times each of those sensors is associated with a PM10 value greater than 50\n",
    "    \n",
    "    \n",
    "* Answer:\n",
    "        (s1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b969a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwser:\n",
      "    ('s1', 175.9)\n",
      "    ('s2', 52.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    }
   ],
   "source": [
    "# Put your code here: \n",
    "# 首先得到每个sensor种大于50的数量\n",
    "\n",
    "def test7(rdd):\n",
    "    def kv(line):\n",
    "        values = line.split(\",\")\n",
    "        return (values[0],float(values[-1]))\n",
    "\n",
    "    def get_num(line):\n",
    "        if line[1] > 50:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    pair = rdd.map(kv)\n",
    "    pair = pair.filter(get_num)\n",
    "    pair = pair.reduceByKey(lambda x,y:x+y)\n",
    "    pair = pair.filter(lambda line: line[1] >= 2)\n",
    "    printRDD(pair)\n",
    "\n",
    "    \n",
    "test7(pmrdd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e30590",
   "metadata": {},
   "source": [
    "### 2.8 Critical dates analysis\n",
    "\n",
    "\n",
    "* Output: an HDFS file containing one line for each sensor\n",
    "    * Each line contains a sensorId and the list of dates with a PM10 value greater than 50 for that sensor\n",
    "\n",
    "\n",
    "* Answer: \n",
    "        s1,['2016-01-01', '2016-01-03', '2016-01-03']\n",
    "        s2,['2016-01-03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "22c60780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwser:\n",
      "    ('s1', <pyspark.resultiterable.ResultIterable object at 0x12f3875e0>)\n",
      "    ('s2', <pyspark.resultiterable.ResultIterable object at 0x12f425370>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n",
      "/opt/miniconda3/envs/myEnv/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:65: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    }
   ],
   "source": [
    "# Put your code here: \n",
    "# 首先筛选出大于50\n",
    "def test8(rdd):\n",
    "    def kv(line):\n",
    "        values = line.split(\",\")\n",
    "        return (line,float(values[-1]))\n",
    "\n",
    "    def kv2(line):\n",
    "        values = line[0].split(\",\")\n",
    "        return (values[0], values[1])\n",
    "    \n",
    "    \n",
    "\n",
    "    pair_rdd = rdd.map(kv).filter(lambda line: line[1] > 50)\n",
    "    pair_rdd = pair_rdd.map(kv2)\n",
    "    pair_rdd = pair_rdd.groupByKey()\n",
    "\n",
    "    printRDD(pair_rdd)\n",
    "\n",
    "    \n",
    "# 如何打印出？\n",
    "test8(pmrdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ed3d4",
   "metadata": {},
   "source": [
    "### 2.9  Order sensors by number of critical days\n",
    "\n",
    "* Output: an HDFS file containing the sensors ordered by the number of critical days\n",
    "\n",
    "    * Each line of the output file contains the number of days with a PM10 value greater than 50 for a sensor s and the sensorId of sensor\n",
    "    \n",
    "\n",
    "* Answer:\n",
    "        3, s1 \n",
    "        1, s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "789942b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'s1': 3, 's2': 1})\n"
     ]
    }
   ],
   "source": [
    "# Put your code here:\n",
    "def test9(rdd):\n",
    "    def kv(line):\n",
    "        values = line.split(\",\")\n",
    "        return (line,float(values[-1]))\n",
    "\n",
    "    def kv2(line):\n",
    "        values = line[0].split(\",\")\n",
    "        return (values[0], values[1])\n",
    "    \n",
    "    \n",
    "\n",
    "    pair_rdd = rdd.map(kv).filter(lambda line: line[1] > 50)\n",
    "    pair_rdd = pair_rdd.map(kv2)\n",
    "    pair_rdd = pair_rdd.countByKey()\n",
    "\n",
    "    print(pair_rdd)\n",
    "\n",
    "    \n",
    "# 如何打印出？\n",
    "test9(pmrdd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbcd2e",
   "metadata": {},
   "source": [
    "### 2.10 Top-k most critical sensors\n",
    "\n",
    "* Output:\n",
    "\n",
    "    * An HDFS file containing the top-k critical sensors\n",
    "\n",
    "        * The “criticality” of a sensor is given by the number of days with a PM10 value greater than 50 \n",
    "        * Each line contains the number of critical days and the sensorId\n",
    "        \n",
    "        \n",
    "* set: k=1\n",
    "\n",
    "* Answer:\n",
    "        3,S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23251514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here: \n",
    "def test9(rdd):\n",
    "    def kv(line):\n",
    "        values = line.split(\",\")\n",
    "        return (line,float(values[-1]))\n",
    "\n",
    "    def kv2(line):\n",
    "        values = line[0].split(\",\")\n",
    "        return (values[0], values[1])\n",
    "    \n",
    "    \n",
    "\n",
    "    pair_rdd = rdd.map(kv).filter(lambda line: line[1] > 50)\n",
    "    pair_rdd = pair_rdd.map(kv2)\n",
    "    pair_rdd = pair_rdd.countByKey()\n",
    "\n",
    "    print(pair_rdd)\n",
    "\n",
    "    \n",
    "# 如何打印出？\n",
    "test9(pmrdd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee58489",
   "metadata": {},
   "source": [
    "## 3. NLP\n",
    "\n",
    "* Input:\n",
    "    * A large textual file containing a set of questions\n",
    "\n",
    "        * Each line contains one question\n",
    "            * Each line has the format:\n",
    "            \n",
    "                    QuestionId,Timestamp,TextOfTheQuestion\n",
    "        \n",
    "    * A large textual file containing a set of answers\n",
    "\n",
    "        * Each line contains one answer \n",
    "        * Each line has the format:\n",
    "        \n",
    "                AnswerId,QuestionId,Timestamp,TextOfTheAnswer\n",
    "                \n",
    "* Datafile Path: ./data/question.txt, ./data/answer.txt\n",
    "\n",
    "\n",
    "* Data:\n",
    "    * Question:\n",
    "            Q1,2015-01-01,What is ..? \n",
    "            Q2,2015-01-03,Who invented ..\n",
    "            \n",
    "    * Answer:\n",
    "            A1,Q1,2015-01-02,It is ..\n",
    "            A2,Q2,2015-01-03,John Smith\n",
    "            A3,Q1,2015-01-05,I think it is .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8f9aec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "qRDD=sc.textFile(\"./data/question.txt\")\n",
    "aRDD=sc.textFile(\"./data/answer.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c8f152",
   "metadata": {},
   "source": [
    "### 3.1 Mapping Question-Answer(s)\n",
    "\n",
    "* Output:\n",
    "\n",
    "    * A file containing one line for each question \n",
    "    * Each line contains a question and the list of answers to that question\n",
    "\n",
    "            QuestionId, TextOfTheQuestion, list of Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9473e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code here \n",
    "def test31(rdd1,rdd2):\n",
    "    def kv1(line):\n",
    "        return(line.split(\",\")[0],line.split(\",\")[-1])\n",
    "\n",
    "    def kv2(line):\n",
    "        return(line.split(\",\")[1],line.split(\",\")[-1])\n",
    "    \n",
    "        \n",
    "    rdd1 = rdd1.map(kv1)\n",
    "    rdd2 = rdd2.map(kv2)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61154135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('myEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8d511ecf47e37874b67868b64cf799c0578a6f729d52bb04263c873a635bf916"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
